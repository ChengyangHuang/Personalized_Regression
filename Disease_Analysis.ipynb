{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Disease Analysis",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPaMkv7sVgd9mumble+9HmQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ChengyangHuang/Personalized_Regression/blob/main/Disease_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1PkK996LJPRh"
      },
      "source": [
        "import os\n",
        "\n",
        "if not os.path.exists(\"/content/hcvdat0.csv\"):\n",
        "    !wget https://archive.ics.uci.edu/ml/machine-learning-databases/00571/hcvdat0.csv"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wRWWorSvKdtB"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.mixture import GaussianMixture, BayesianGaussianMixture\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score\n",
        "from sklearn.impute import KNNImputer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from scipy.spatial import KDTree\n",
        "from scipy.spatial import distance_matrix\n",
        "\n",
        "import torch\n",
        "\n",
        "# np.random.seed(0)\n",
        "\n",
        "\n",
        "def load_dataset(impute=True, binary=True):\n",
        "    dataset = pd.read_csv(\"/content/hcvdat0.csv\", usecols=range(1, 14))\n",
        "    X = []\n",
        "    sex_dict = {'m': 0,\n",
        "                'f': 1}\n",
        "    for name, data in dataset.items():\n",
        "        if name == \"Category\":\n",
        "            y = data.to_list()\n",
        "            y = [x.split('=')[0] for x in y]\n",
        "            y = np.array([int(x) if x.isdigit() else 5 for x in y])\n",
        "            if binary:\n",
        "                y = np.array([0 if x == 0 or x == 5 else 1 for x in y])\n",
        "        elif name == \"Sex\":\n",
        "            X.append([int(sex_dict[x]) for x in data])\n",
        "        else:\n",
        "            X.append([float(x) for x in data])\n",
        "\n",
        "    X = np.array(X).T\n",
        "    if impute == True:\n",
        "        imputer = KNNImputer(n_neighbors=2)\n",
        "        X = imputer.fit_transform(X)\n",
        "    return X, y\n",
        "\n",
        "\n",
        "def plot_parameters(theta_gt, theta_est):\n",
        "    fig, ax = plt.subplots(1)\n",
        "    ax.scatter(theta_gt[:, 0], theta_gt[:, 1], label=\"True Parameters\")\n",
        "    ax.scatter(theta_est[:, 0], theta_est[:, 1], label=\"EST. Parameters\")\n",
        "    ax.legend()\n",
        "    return fig, ax\n",
        "\n",
        "\n",
        "def print_metrics(dic):\n",
        "    message = f\"\\n\"\n",
        "    for k, v in dic.items():\n",
        "        message += f\"{k}:\\t{v}\\n\"\n",
        "    print(message)\n",
        "\n",
        "\n",
        "def evaluate_method(method, X_test, y_test, method_name):\n",
        "    y_pred = method.predict(X_test)\n",
        "    y_prob = method.predict_proba(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    precision, recall, fscore, _ = precision_recall_fscore_support(y_test, y_pred)\n",
        "    auroc = roc_auc_score(y_test, y_prob[:, 1])\n",
        "    performance_dict = {\"method_name\": method_name,\n",
        "                        \"accuracy\": accuracy,\n",
        "                        \"precision\": precision,\n",
        "                        \"recall\": recall,\n",
        "                        \"fscore\": fscore,\n",
        "                        \"auroc\": auroc}\n",
        "    return performance_dict"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JFhhox9rqDvy"
      },
      "source": [
        "class PR():\n",
        "    def __init__(self, theta_pop, dataset, args):\n",
        "        # Training Parameters\n",
        "        self.sigma_theta = args.sigma_theta\n",
        "        self.lambd = args.theta_regularizer\n",
        "        self.gamma = args.distance_regularizer\n",
        "        self.nu = args.phi_regularizer\n",
        "        self.alpha = args.learning_rate\n",
        "        self.c = args.lr_decay\n",
        "        self.q = args.latent_dim\n",
        "        self.log_steps = args.log_steps\n",
        "        self.n_neighbors = args.n_neighbors\n",
        "        self.show_logs = args.show_logs\n",
        "\n",
        "        self.theta_pop = torch.from_numpy(theta_pop)\n",
        "        self.p = self.theta_pop.shape[0]\n",
        "\n",
        "        # Training Data\n",
        "        X_train, y_train, theta_train, U_train = dataset\n",
        "        self.X_train = torch.from_numpy(X_train)\n",
        "        self.y_train = torch.from_numpy(y_train)\n",
        "        self.U_train = torch.from_numpy(U_train)\n",
        "        self.U_tree = KDTree(U_train) \n",
        "        self.U_distance_mat = torch.from_numpy(distance_matrix(U_train, U_train, p=2))\n",
        "        self.theta_train = torch.from_numpy(theta_train)\n",
        "        self.n = self.X_train.shape[0]\n",
        "        self.k = self.U_train.shape[1]\n",
        "\n",
        "        # Variable Initialization\n",
        "        PI = np.random.multivariate_normal(theta_pop, self.sigma_theta*np.eye(self.p), size=self.n)\n",
        "        self.PI = torch.from_numpy(PI)\n",
        "        self.Z, self.Q = self.__init_ZnQ(PI)\n",
        "        self.phi = torch.ones((self.k, ))\n",
        "\n",
        "        # Loss Functions\n",
        "        self.__sample_specific_loss = torch.nn.MSELoss(reduction='sum')\n",
        "        self.__parameter_regularizer = torch.nn.L1Loss()\n",
        "        self.__phi_regularizer = torch.nn.MSELoss()\n",
        "        self.__distance_loss = torch.nn.MSELoss()\n",
        "\n",
        "\n",
        "    def train(self, n_epoch=5):\n",
        "        for epoch in range(n_epoch):\n",
        "            Z = self.Z.clone().requires_grad_(True)\n",
        "            Q = self.Q.clone().requires_grad_(True)\n",
        "            phi = self.phi.clone().requires_grad_(True)\n",
        "            PI = self.__update_PI(Z, Q) \n",
        "\n",
        "            y_pred = (self.X_train * PI).sum(dim=1, keepdim=True)\n",
        "\n",
        "            # Calculate Loss\n",
        "            l_loss = self.__sample_specific_loss(y_pred, self.y_train)\n",
        "            # D_loss = self.__distance_matching_regularizer(Z, phi)\n",
        "            D_loss = 0\n",
        "            theta_loss = self.__parameter_regularizer(PI, torch.zeros_like(PI))\n",
        "            phi_loss = self.__phi_regularizer(phi, torch.ones_like(phi))\n",
        "            loss = l_loss + self.gamma * D_loss + self.lambd * theta_loss + self.nu * phi_loss\n",
        "            \n",
        "            # Update phi\n",
        "            loss.backward()\n",
        "            self.phi -= self.alpha * phi.grad\n",
        "\n",
        "            # Update Z\n",
        "            alpha_cust = self.alpha / torch.linalg.norm(self.PI-self.theta_pop, \n",
        "                                                        float('inf'), dim=1, keepdim=True)\n",
        "            self.Z -= alpha_cust * Z.grad\n",
        "\n",
        "            # Update Q\n",
        "            self.Q -= self.alpha * Q.grad\n",
        "\n",
        "            # Update alpha\n",
        "            self.alpha *= self.c\n",
        "\n",
        "            # Update theta, PI\n",
        "            self.PI = self.__update_PI(self.Z, self.Q)\n",
        "\n",
        "            if self.show_logs and epoch % self.log_steps == 0:\n",
        "                message = f\"Epoch {epoch+1} - Total loss: {loss}\\tDistance loss:{D_loss}\"\n",
        "                print(message)\n",
        "                plot_parameters(self.theta_train, self.PI)\n",
        "                plt.show()\n",
        "        \n",
        "\n",
        "    def predict(self, X_test, U_test):\n",
        "        X = torch.from_numpy(X_test)\n",
        "        _, idx_sets = self.U_tree.query(U_test, k=self.n_neighbors)\n",
        "        y_pred = []\n",
        "        for i in range(X_test.shape[0]):\n",
        "            idxs = idx_sets[i]\n",
        "            theta = self.PI[idxs, :].mean(axis=0)\n",
        "            y = torch.dot(X[i], theta).numpy()\n",
        "            y_pred.append(y)  \n",
        "        return y_pred\n",
        "\n",
        "\n",
        "    def __init_ZnQ(self, PI):\n",
        "        pca = PCA(n_components=self.q, whiten=False)\n",
        "        Z = pca.fit_transform(PI)\n",
        "        Q = pca.components_\n",
        "        return torch.from_numpy(Z), torch.from_numpy(Q)\n",
        "    \n",
        "\n",
        "    def __update_PI(self, Z, Q):\n",
        "        return torch.mm(Z, Q) + self.theta_pop\n",
        "    \n",
        "\n",
        "    def __distance_matching_regularizer(self, Z, phi):\n",
        "        #TODO\n",
        "        \n",
        "        _Z = Z.detach().numpy()\n",
        "        tree = KDTree(_Z)\n",
        "        _, J_sets = tree.query(_Z, k=6)\n",
        "        d_loss = 0\n",
        "        for i in range(J_sets.shape[0]):\n",
        "            J_set = J_sets[i][1:]\n",
        "            dU = phi * self.U_distance_mat[i, J_set]\n",
        "            dZ = torch.linalg.norm(Z[i] - Z[J_set, :], ord=2, dim=1)\n",
        "            d_loss += self.__distance_loss(dZ, dU)\n",
        "        return d_loss\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uSwrQf_7prQN",
        "outputId": "c9717cf7-d492-462b-da6e-15e5a4b053e7"
      },
      "source": [
        "class PR_Arguments():\n",
        "    sigma_theta = 0.001\n",
        "    learning_rate = 4e-3\n",
        "    lr_decay = 1-1e-4\n",
        "    latent_dim = 2\n",
        "    n_epoch = 2000 #2000\n",
        "    theta_regularizer = 0.01\n",
        "    distance_regularizer = 0.01\n",
        "    phi_regularizer = 0\n",
        "\n",
        "    n_neighbors = 3\n",
        "    log_steps = 100\n",
        "    show_logs = False\n",
        "\n",
        "\n",
        "def main():\n",
        "    X, y = load_dataset()\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
        "\n",
        "    # dataset_train = (X_train, y_train, theta_train, U_train)\n",
        "\n",
        "    dict_list = []\n",
        "    ## Logistic Regression\n",
        "    lr = LogisticRegression().fit(X_train, y_train)\n",
        "    dict_lr = evaluate_method(lr, X_test, y_test, \"Logistic Regression\")\n",
        "    dict_list.append(dict_lr)\n",
        "    # theta_lr = np.array(lr.coef_)\n",
        "    # print(theta_lr)\n",
        "    # plot_parameters(theta_train, theta_lr)\n",
        "\n",
        "    # ## Other Baseline methods\n",
        "    # DNN\n",
        "    dnn = MLPClassifier(hidden_layer_sizes=(50,)).fit(X_train, y_train.squeeze())\n",
        "    dict_dnn = evaluate_method(dnn, X_test, y_test, \"Deep Neural Networks\")\n",
        "    dict_list.append(dict_dnn)\n",
        "    theta_dnn = np.array(dnn.coefs_[0].T)\n",
        "    # print(theta_dnn.shape)\n",
        "    # plot_parameters(theta_train, theta_dnn)\n",
        "\n",
        "    # ## Personalized Regression\n",
        "    # pr_args = PR_Arguments()\n",
        "    # pr = PR(theta_lr.squeeze(), dataset_train, pr_args)\n",
        "    # pr.train(pr_args.n_epoch)\n",
        "    # y_pred_pr = pr.predict(X_test, U_test)\n",
        "    # mse_pr = mean_squared_error(y_test, y_pred_pr)\n",
        "    # plot_parameters(theta_train, pr.PI)\n",
        "    \n",
        "\n",
        "    for x in dict_list:\n",
        "        print_metrics(x)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "method_name:\tLogistic Regression\n",
            "accuracy:\t0.9113300492610837\n",
            "precision:\t[0.91847826 0.84210526]\n",
            "recall:\t[0.98255814 0.51612903]\n",
            "fscore:\t[0.9494382 0.64     ]\n",
            "auroc:\t0.9253563390847712\n",
            "\n",
            "\n",
            "method_name:\tDeep Neural Networks\n",
            "accuracy:\t0.9507389162561576\n",
            "precision:\t[0.96022727 0.88888889]\n",
            "recall:\t[0.98255814 0.77419355]\n",
            "fscore:\t[0.97126437 0.82758621]\n",
            "auroc:\t0.9351087771942985\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}