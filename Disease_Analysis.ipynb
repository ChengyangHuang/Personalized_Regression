{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Disease Analysis",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ChengyangHuang/Personalized_Regression/blob/main/Disease_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1PkK996LJPRh"
      },
      "source": [
        "import os\n",
        "\n",
        "if not os.path.exists(\"/content/hcvdat0.csv\"):\n",
        "    !wget https://archive.ics.uci.edu/ml/machine-learning-databases/00571/hcvdat0.csv"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wRWWorSvKdtB"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.impute import KNNImputer\n",
        "\n",
        "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
        "from sklearn.mixture import GaussianMixture, BayesianGaussianMixture\n",
        "from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score\n",
        "\n",
        "from scipy.spatial import KDTree\n",
        "from scipy.spatial import distance_matrix\n",
        "from scipy.special import softmax, expit\n",
        "\n",
        "import torch\n",
        "\n",
        "# np.random.seed(0)\n",
        "\n",
        "\n",
        "def load_dataset(impute=True, binary=True):\n",
        "    dataset = pd.read_csv(\"/content/hcvdat0.csv\", usecols=range(1, 14))\n",
        "    print(dataset)\n",
        "    X = []\n",
        "    sex_dict = {'m': 0,\n",
        "                'f': 1}\n",
        "    for name, data in dataset.items():\n",
        "        if name == \"Category\":\n",
        "            y = data.to_list()\n",
        "            y = [x.split('=')[0] for x in y]\n",
        "            y = np.array([int(x) if x.isdigit() else 5 for x in y])\n",
        "            if binary:\n",
        "                y = np.array([0 if x == 0 or x == 5 else 1 for x in y])\n",
        "        elif name == \"Sex\":\n",
        "            X.append([int(sex_dict[x]) for x in data])\n",
        "        else:\n",
        "            X.append([float(x) for x in data])\n",
        "\n",
        "    X = np.array(X).T\n",
        "    if impute == True:\n",
        "        imputer = KNNImputer(n_neighbors=2)\n",
        "        X = imputer.fit_transform(X)\n",
        "    return X, y\n",
        "\n",
        "\n",
        "def plot_parameters(theta_gt, theta_est):\n",
        "    fig, ax = plt.subplots(1)\n",
        "    ax.scatter(theta_gt[:, 0], theta_gt[:, 1], label=\"True Parameters\")\n",
        "    ax.scatter(theta_est[:, 0], theta_est[:, 1], label=\"EST. Parameters\")\n",
        "    ax.legend()\n",
        "    return fig, ax\n",
        "\n",
        "\n",
        "def print_metrics(dic):\n",
        "    message = f\"\\n\"\n",
        "    for k, v in dic.items():\n",
        "        message += f\"{k}:\\t{v}\\n\"\n",
        "    print(message)\n",
        "\n",
        "\n",
        "def evaluate_method(method, X_test, y_test, method_name, U_test=None):\n",
        "    if method_name == \"Linear Regression\" or method_name == \"Gaussian Mixture\":\n",
        "        pred = method.predict(X_test)\n",
        "        y_pred = (pred>0.5)*1\n",
        "        # y_prob = expit(pred).reshape((-1, 1))\n",
        "        y_prob = np.clip(pred, 0, 1).reshape((-1, 1))\n",
        "        y_prob = np.concatenate((1-y_prob, y_prob), axis=1)\n",
        "    elif method_name == \"Personalized Regression\":\n",
        "        y_pred = method.predict(X_test, U_test)\n",
        "        y_prob = method.predict_proba(X_test, U_test)\n",
        "    else:\n",
        "        y_pred = method.predict(X_test)\n",
        "        y_prob = method.predict_proba(X_test)\n",
        "    \n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    precision, recall, fscore, _ = precision_recall_fscore_support(y_test, y_pred)\n",
        "    auroc = roc_auc_score(y_test, y_prob[:, 1])\n",
        "    performance_dict = {\"method_name\": method_name,\n",
        "                        \"accuracy\": accuracy,\n",
        "                        \"precision\": precision,\n",
        "                        \"recall\": recall,\n",
        "                        \"fscore\": fscore,\n",
        "                        \"auroc\": auroc}\n",
        "    return performance_dict\n",
        "\n",
        "\n"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JFhhox9rqDvy"
      },
      "source": [
        "class PRBinaryClassifier():\n",
        "    def __init__(self, args, theta_pop, dataset, U_train=None):\n",
        "        # Training Parameters\n",
        "        self.cov_norm_ord = args.covariate_norm_ord\n",
        "        self.cov_xstart = args.covariate_x_start\n",
        "        self.cov_xend= args.covariate_x_end\n",
        "        self.sigma_theta = args.sigma_theta\n",
        "        self.lambd = args.theta_regularizer\n",
        "        self.gamma = args.distance_regularizer\n",
        "        self.nu = args.phi_regularizer\n",
        "        self.alpha = args.learning_rate\n",
        "        self.c = args.lr_decay\n",
        "        self.q = args.latent_dim\n",
        "        self.k = args.covariate_dim\n",
        "        self.log_steps = args.log_steps\n",
        "        self.n_neighbors = args.n_neighbors\n",
        "        self.show_logs = args.show_logs\n",
        "        self.use_distance_loss = args.use_distance_loss\n",
        "\n",
        "        self.theta_pop = torch.from_numpy(theta_pop)\n",
        "        self.p = self.theta_pop.shape[0]\n",
        "\n",
        "        # Training Data\n",
        "        X_train, y_train = dataset\n",
        "        self.X_train = torch.from_numpy(X_train)\n",
        "        self.y_train = torch.from_numpy(y_train).to(dtype=torch.double)\n",
        "        self.__init_U_train(X_train, U_train)\n",
        "        self.n = self.X_train.shape[0]\n",
        "\n",
        "        # Variable Initialization\n",
        "        PI = np.random.multivariate_normal(theta_pop, self.sigma_theta*np.eye(self.p), size=self.n)\n",
        "        self.PI = torch.from_numpy(PI)\n",
        "        self.Z, self.Q = self.__init_ZnQ(PI)\n",
        "        self.phi = torch.ones((self.k, ), dtype=torch.float64) / self.k\n",
        "\n",
        "        # Loss Functions\n",
        "        self.__sample_specific_loss = torch.nn.BCELoss(reduction=\"sum\")\n",
        "        self.__parameter_regularizer = torch.nn.L1Loss()\n",
        "        self.__phi_regularizer = torch.nn.MSELoss()\n",
        "        self.__distance_loss = torch.nn.MSELoss()\n",
        "\n",
        "\n",
        "    def train(self, n_epoch=5):\n",
        "        for epoch in range(n_epoch):\n",
        "            # Require Gradient for variables\n",
        "            Z = self.Z.clone().requires_grad_(True)\n",
        "            Q = self.Q.clone().requires_grad_(True)\n",
        "            phi = self.phi.clone().requires_grad_(True)\n",
        "            PI = self.__update_PI(Z, Q) \n",
        "\n",
        "            y_prob = self.__predict_proba(self.X_train, PI)\n",
        "\n",
        "            # Calculate Loss\n",
        "            l_loss = self.__sample_specific_loss(y_prob, self.y_train)\n",
        "            if self.use_distance_loss:\n",
        "                D_loss = self.__distance_matching_regularizer(Z, phi)\n",
        "            else:\n",
        "                D_loss = 0  \n",
        "            theta_loss = self.__parameter_regularizer(PI, torch.zeros_like(PI))\n",
        "            phi_sum = torch.sum(phi, dim=0, keepdim=True)\n",
        "            phi_loss = self.__phi_regularizer(phi_sum, torch.ones_like(phi_sum)) # Weight should sum to one\n",
        "            loss = l_loss + self.gamma * D_loss + self.lambd * theta_loss + self.nu * phi_loss\n",
        "            \n",
        "            # Update phi\n",
        "            loss.backward()\n",
        "            self.phi -= self.alpha * phi.grad\n",
        "\n",
        "            # Update Z\n",
        "            alpha_cust = self.alpha / torch.linalg.norm(self.PI-self.theta_pop, \n",
        "                                                        float('inf'), dim=1, keepdim=True)\n",
        "            self.Z -= alpha_cust * Z.grad\n",
        "\n",
        "            # Update Q\n",
        "            self.Q -= self.alpha * Q.grad\n",
        "\n",
        "            # Update alpha\n",
        "            self.alpha *= self.c\n",
        "\n",
        "            # Update theta, PI\n",
        "            self.PI = self.__update_PI(self.Z, self.Q)\n",
        "\n",
        "            if self.show_logs and epoch % self.log_steps == 0:\n",
        "                message = f\"Epoch {epoch+1} - Total loss: {loss}\\tDistance loss:{D_loss}\"\n",
        "                print(message)\n",
        "        \n",
        "\n",
        "    def predict(self, X_test, U_test=None):\n",
        "        y_prob = self.predict_proba(X_test, U_test)\n",
        "        y_pred = y_prob.argmax(axis=1)\n",
        "        return y_pred\n",
        "\n",
        "\n",
        "    def predict_proba(self, X_test, U_test=None):\n",
        "        if type(U_test) == \"NoneType\":\n",
        "            U_test = self.U_trans.fit_transform(X_test)\n",
        "        X = torch.from_numpy(X_test)\n",
        "        # TODO: Query by weighted norm\n",
        "        # _, idx_sets = self.U_tree.query(U_test, k=self.n_neighbors)\n",
        "        dist_mat_test = self.__create_distance_matrix(self.U_train.numpy(), U_test)\n",
        "        _, idx_sets = torch.matmul(dist_mat_test, self.phi).topk(self.n_neighbors, dim=1)\n",
        "        PI = self.PI[idx_sets, :].mean(axis=1)\n",
        "        prob = self.__predict_proba(X, PI).detach().numpy().reshape((-1, 1))\n",
        "        y_prob = np.concatenate((1-prob, prob), axis=1)\n",
        "        return y_prob\n",
        "\n",
        "\n",
        "    def __predict_proba(self, X, PI):\n",
        "        prob = torch.sigmoid((X * PI).sum(dim=1, keepdim=False))\n",
        "        return prob\n",
        "\n",
        "    def __create_distance_matrix(self, A, B):\n",
        "        dist_mat = []\n",
        "        for i in range(self.k):\n",
        "            dist_mat.append(distance_matrix(A[:, [i]], B[:, [i]], p=self.cov_norm_ord))\n",
        "        dist_mat = torch.from_numpy(np.array(dist_mat)).T\n",
        "        return dist_mat\n",
        "\n",
        "    def __init_U_train(self, X_train: np.array, U_train=None):\n",
        "        if type(U_train) == \"NoneType\":\n",
        "            self.U_trans = TSNE(n_components=self.k, n_iter=1000)\n",
        "            U_train = self.U_trans.fit_transform(X_train[:, self.cov_xstart:self.cov_xend])\n",
        "        self.U_train = torch.from_numpy(U_train)\n",
        "        # TODO: Query by weighted norm, once done remove U_tree\n",
        "        # self.U_tree = KDTree(U_train) \n",
        "        U_dist_mat = []\n",
        "        for i in range(self.k):\n",
        "            U_dist_mat.append(distance_matrix(U_train[:, [i]], U_train[:, [i]], p=self.cov_norm_ord))\n",
        "        self.U_distance_mat = torch.from_numpy(np.array(U_dist_mat)).T # Should be fine since distance is symmetry\n",
        "        # print(f\"Distance Matrix Shape: {self.U_distance_mat.shape}\")\n",
        "\n",
        "\n",
        "    def __init_ZnQ(self, PI):\n",
        "        pca = PCA(n_components=self.q, whiten=False)\n",
        "        Z = pca.fit_transform(PI)\n",
        "        Q = pca.components_\n",
        "        return torch.from_numpy(Z), torch.from_numpy(Q)\n",
        "    \n",
        "\n",
        "    def __update_PI(self, Z, Q):\n",
        "        return torch.mm(Z, Q) + self.theta_pop\n",
        "    \n",
        "\n",
        "    def __distance_matching_regularizer(self, Z, phi):\n",
        "        #TODO\n",
        "        \n",
        "        _Z = Z.detach().numpy()\n",
        "        tree = KDTree(_Z)\n",
        "        _, idx_set = tree.query(_Z, k=6)\n",
        "        idx_set = idx_set[:, 1:]\n",
        "        Z_set = Z[idx_set, :]\n",
        "        delta_Z = Z.view(Z.shape[0], 1, -1).expand(-1, Z_set.shape[1], -1) - Z_set\n",
        "        delta_Z = torch.linalg.norm(delta_Z, ord=2, dim=2)\n",
        "\n",
        "        U_dist_idx = torch.from_numpy(idx_set).to(dtype=torch.int64).view(idx_set.shape[0], -1, 1).expand(-1, -1, self.k)\n",
        "        U_dist_set = torch.gather(self.U_distance_mat, 1, U_dist_idx)\n",
        "        rho_U = torch.matmul(U_dist_set, phi)\n",
        "        d_loss = self.__distance_loss(delta_Z, rho_U)\n",
        "        return d_loss\n"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uSwrQf_7prQN",
        "outputId": "63dcf56b-6fa0-413c-d84d-e2f798994ced"
      },
      "source": [
        "class PR_Arguments():\n",
        "    # Data Arguments\n",
        "    sigma_theta = 0.001\n",
        "    theta_regularizer = 0.01\n",
        "    distance_regularizer = 0.01\n",
        "    phi_regularizer = 1\n",
        "\n",
        "    # \n",
        "    n_neighbors = 3\n",
        "    latent_dim = 2\n",
        "    covariate_dim = 2\n",
        "    covariate_norm_ord = 2\n",
        "    covariate_x_start = 2\n",
        "    covariate_x_end = -1\n",
        "\n",
        "    # Traing Arguments\n",
        "    learning_rate = 4e-3\n",
        "    lr_decay = 1-1e-4\n",
        "    n_epoch = 3000 #2000\n",
        "    log_steps = 100\n",
        "    show_logs = True\n",
        "    use_distance_loss = False\n",
        "\n",
        "\n",
        "def main():\n",
        "    X, y = load_dataset()\n",
        "    U = TSNE().fit_transform(X[:, 2:])\n",
        "    X_train, X_test, y_train, y_test, U_train, U_test = train_test_split(X, y, U, test_size=0.33, random_state=42)\n",
        "\n",
        "    ## Preprocessing\n",
        "    scaler =  StandardScaler()\n",
        "    X_train = scaler.fit_transform(X_train)\n",
        "    X_test = scaler.transform(X_test)\n",
        "\n",
        "    ### Methods\n",
        "    dict_list = []\n",
        "\n",
        "    ## Linear Regression\n",
        "    lr = LinearRegression().fit(X_train, y_train)\n",
        "    dict_lr = evaluate_method(lr, X_test, y_test, \"Linear Regression\")\n",
        "    dict_list.append(dict_lr)\n",
        "\n",
        "    ## Gaussian Mixture\n",
        "    gmm = GaussianMixture(n_components=3).fit(X_train, y_train)\n",
        "    dict_gmm = evaluate_method(gmm, X_test, y_test, \"Gaussian Mixture\")\n",
        "    dict_list.append(dict_gmm)\n",
        "\n",
        "    ## DNN\n",
        "    dnn = MLPClassifier(hidden_layer_sizes=(50,)).fit(X_train, y_train.squeeze())\n",
        "    dict_dnn = evaluate_method(dnn, X_test, y_test, \"Deep Neural Networks\")\n",
        "    dict_list.append(dict_dnn)\n",
        "\n",
        "    ## Logistic Regression\n",
        "    lor = LogisticRegression().fit(X_train, y_train)\n",
        "    dict_lor = evaluate_method(lor, X_test, y_test, \"Logistic Regression\")\n",
        "    dict_list.append(dict_lor)\n",
        "\n",
        "    ## Personalized Regression\n",
        "    pr_args = PR_Arguments()\n",
        "    theta_lor = np.concatenate((np.array(lor.coef_).T, np.array(lor.intercept_).reshape((-1, 1))), axis=0)\n",
        "    X_train = np.concatenate((X_train, np.ones((X_train.shape[0], 1))), axis=1)\n",
        "    X_test = np.concatenate((X_test, np.ones((X_test.shape[0], 1))), axis=1)\n",
        "    dataset_train = (X_train, y_train)\n",
        "    pr = PRBinaryClassifier(pr_args, theta_lor.squeeze(), dataset_train, U_train)\n",
        "    pr.train(pr_args.n_epoch)\n",
        "    dict_pr = evaluate_method(pr, X_test, y_test, \"Personalized Regression\", U_test)\n",
        "    dict_list.append(dict_pr) \n",
        "\n",
        "    for x in dict_list:\n",
        "        print_metrics(x)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "          Category  Age Sex   ALB    ALP  ...    CHE  CHOL   CREA    GGT  PROT\n",
            "0    0=Blood Donor   32   m  38.5   52.5  ...   6.93  3.23  106.0   12.1  69.0\n",
            "1    0=Blood Donor   32   m  38.5   70.3  ...  11.17  4.80   74.0   15.6  76.5\n",
            "2    0=Blood Donor   32   m  46.9   74.7  ...   8.84  5.20   86.0   33.2  79.3\n",
            "3    0=Blood Donor   32   m  43.2   52.0  ...   7.33  4.74   80.0   33.8  75.7\n",
            "4    0=Blood Donor   32   m  39.2   74.1  ...   9.15  4.32   76.0   29.9  68.7\n",
            "..             ...  ...  ..   ...    ...  ...    ...   ...    ...    ...   ...\n",
            "610    3=Cirrhosis   62   f  32.0  416.6  ...   5.57  6.30   55.7  650.9  68.5\n",
            "611    3=Cirrhosis   64   f  24.0  102.8  ...   1.54  3.02   63.0   35.9  71.3\n",
            "612    3=Cirrhosis   64   f  29.0   87.3  ...   1.66  3.63   66.7   64.2  82.0\n",
            "613    3=Cirrhosis   46   f  33.0    NaN  ...   3.56  4.20   52.0   50.0  71.0\n",
            "614    3=Cirrhosis   59   f  36.0    NaN  ...   9.07  5.30   67.0   34.0  68.0\n",
            "\n",
            "[615 rows x 13 columns]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 - Total loss: 36.457227717848845\tDistance loss:0\n",
            "Epoch 101 - Total loss: 13.690198661972\tDistance loss:0\n",
            "Epoch 201 - Total loss: 5.004893438640249\tDistance loss:0\n",
            "Epoch 301 - Total loss: 3.4648523900427968\tDistance loss:0\n",
            "Epoch 401 - Total loss: 2.7602754367209195\tDistance loss:0\n",
            "Epoch 501 - Total loss: 2.3252808458487175\tDistance loss:0\n",
            "Epoch 601 - Total loss: 2.0237172769034832\tDistance loss:0\n",
            "Epoch 701 - Total loss: 1.7998825560914835\tDistance loss:0\n",
            "Epoch 801 - Total loss: 1.626014137675578\tDistance loss:0\n",
            "Epoch 901 - Total loss: 1.4864443502905684\tDistance loss:0\n",
            "Epoch 1001 - Total loss: 1.3715990356693326\tDistance loss:0\n",
            "Epoch 1101 - Total loss: 1.2751995991896043\tDistance loss:0\n",
            "Epoch 1201 - Total loss: 1.193031426279905\tDistance loss:0\n",
            "Epoch 1301 - Total loss: 1.122080139557273\tDistance loss:0\n",
            "Epoch 1401 - Total loss: 1.0601157223034956\tDistance loss:0\n",
            "Epoch 1501 - Total loss: 1.005485584611354\tDistance loss:0\n",
            "Epoch 1601 - Total loss: 0.9569298101849159\tDistance loss:0\n",
            "Epoch 1701 - Total loss: 0.9134623234675889\tDistance loss:0\n",
            "Epoch 1801 - Total loss: 0.8743066895868431\tDistance loss:0\n",
            "Epoch 1901 - Total loss: 0.8388431365151368\tDistance loss:0\n",
            "Epoch 2001 - Total loss: 0.8065716847152798\tDistance loss:0\n",
            "Epoch 2101 - Total loss: 0.7770717377868126\tDistance loss:0\n",
            "Epoch 2201 - Total loss: 0.7499942286689106\tDistance loss:0\n",
            "Epoch 2301 - Total loss: 0.7250478800196637\tDistance loss:0\n",
            "Epoch 2401 - Total loss: 0.7019869008714155\tDistance loss:0\n",
            "Epoch 2501 - Total loss: 0.680602522393285\tDistance loss:0\n",
            "Epoch 2601 - Total loss: 0.6607183672094074\tDistance loss:0\n",
            "Epoch 2701 - Total loss: 0.6421807367659501\tDistance loss:0\n",
            "Epoch 2801 - Total loss: 0.6248561632861404\tDistance loss:0\n",
            "Epoch 2901 - Total loss: 0.6086285370909514\tDistance loss:0\n",
            "tensor([0.5000, 0.5000], dtype=torch.float64)\n",
            "\n",
            "method_name:\tLinear Regression\n",
            "accuracy:\t0.8817733990147784\n",
            "precision:\t[0.87755102 1.        ]\n",
            "recall:\t[1.         0.22580645]\n",
            "fscore:\t[0.93478261 0.36842105]\n",
            "auroc:\t0.9705551387846961\n",
            "\n",
            "\n",
            "method_name:\tGaussian Mixture\n",
            "accuracy:\t0.39408866995073893\n",
            "precision:\t[0.96226415 0.19333333]\n",
            "recall:\t[0.29651163 0.93548387]\n",
            "fscore:\t[0.45333333 0.32044199]\n",
            "auroc:\t0.6159977494373593\n",
            "\n",
            "\n",
            "method_name:\tDeep Neural Networks\n",
            "accuracy:\t0.9211822660098522\n",
            "precision:\t[0.91935484 0.94117647]\n",
            "recall:\t[0.99418605 0.51612903]\n",
            "fscore:\t[0.95530726 0.66666667]\n",
            "auroc:\t0.9838709677419355\n",
            "\n",
            "\n",
            "method_name:\tLogistic Regression\n",
            "accuracy:\t0.9064039408866995\n",
            "precision:\t[0.90909091 0.875     ]\n",
            "recall:\t[0.98837209 0.4516129 ]\n",
            "fscore:\t[0.94707521 0.59574468]\n",
            "auroc:\t0.9716804201050263\n",
            "\n",
            "\n",
            "method_name:\tPersonalized Regression\n",
            "accuracy:\t0.9310344827586207\n",
            "precision:\t[0.93406593 0.9047619 ]\n",
            "recall:\t[0.98837209 0.61290323]\n",
            "fscore:\t[0.96045198 0.73076923]\n",
            "auroc:\t0.9669917479369843\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}